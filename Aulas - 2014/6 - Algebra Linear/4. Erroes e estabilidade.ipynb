{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plano de hoje\n",
    "-------------\n",
    "\n",
    "1. Ambiente de programação\n",
    "2. Usando o computador para calcular    \n",
    "3. Usando o computador para desenhar\n",
    "4. Usando o computador para integrar: quadraturas\n",
    "5. Usando o computador para aproximar: interpolação\n",
    "6. Álgebra linear computacional\n",
    "    1. Resolvendo sistemas lineares\n",
    "    2. Fatoração\n",
    "    3. Autovalores e autovetores\n",
    "    4. **Erros e estabilidade**\n",
    "    5. **Métodos iterativos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erros e estabilidade de $Ax = b$\n",
    "\n",
    "Ao resolver um sistema linear, pode haver erros numéricos ou erros de medida.\n",
    "Vamos apenas observar um exemplo em que o modelo é relativamente simples.\n",
    "\n",
    "Suponha que, em vez de termos medido $b$ exatamente, temos $b + e$, onde $e$ é um (pequeno!) erro.\n",
    "Isso quer dizer, em geral, que $\\displaystyle \\frac{\\lVert e\\rVert}{\\lVert b\\rVert} << 1$.\n",
    "\n",
    "Seja, agora, $x$ a solução \"real\" de $Ax = b$, e $y$ a solução de $Ay = (b+e)$.\n",
    "Qual o erro?\n",
    "Ou seja, como podemos avaliar $\\displaystyle \\frac{\\lVert (y - x)\\rVert}{\\lVert x\\rVert}$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituindo as equações de $x$ e $y$, temos que $A(y - x) = e$, e portanto $y - x = A^{-1}e$.\n",
    "Assim, temos\n",
    "$$ \\frac{\\lVert (y - x)\\rVert}{\\lVert x\\rVert} = \\frac{\\lVert A^{-1}e\\rVert}{\\lVert A^{-1}b\\rVert} .$$\n",
    "\n",
    "Pensando que $A^{-1}$ vai \"dilatar\" alguns vetores e \"contrair\" outros (mas não sabemos quais!),\n",
    "o que podemos dizer, com certeza, é que a dilatação é menor do que $\\lambda_{\\text{max}}$\n",
    "e maior do que $\\lambda_{\\text{min}}$.\n",
    "Se $A$ for simétrica e positiva definida, estes serão o maior e o menor autovalor, respectivamente.\n",
    "\n",
    "Só que pode ser que o sistema conspire contra nós: $b$ pode estar na direção de menor dilatação,\n",
    "e $e$ na de maior dilatação.\n",
    "Assim, mesmo no pior dos casos, temos:\n",
    "$$ \\frac{\\lVert (y - x)\\rVert}{\\lVert x\\rVert}\n",
    "   \\le \\frac{\\lambda _ {\\text{max}}}{\\lambda _ {\\text{min}}} \\frac{\\lVert e\\rVert}{\\lVert b\\rVert}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Número de condicionamento\n",
    "\n",
    "Vemos que o erro será propagado de $b$ para $x$, e sofrerá um aumento (no máximo) proporcional a\n",
    "$\\frac{\\lambda _ {\\text{max}}}{\\lambda _ {\\text{min}}}$.\n",
    "Esse valor é chamado de número de condicionamento de uma matriz, e,\n",
    "se for muito grande, indica que o sistema que estamos resolvendo será muito \"instável\":\n",
    "pequenos erros de medida podem ser magnificados extraordinariamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Calcule o número de condicionamento da matrix de Hilbert, em função do seu tamanho.\n",
    "Lembre, a matriz é formada por $a _ {i,j} = \\frac{1}{i + j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hilbert(n):\n",
    "    m = []\n",
    "    for i in range(1,n+1):\n",
    "        m.append([1/(i+j) for j in range(1,n+1)])\n",
    "    return array(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos iterativos\n",
    "\n",
    "Uma outra forma de resolver sistemas lineares, além das que já vimos,\n",
    "se baseia em métodos iterativos.\n",
    "Da mesma forma que os métodos iterativos para encontrar raízes,\n",
    "a sua grande utilidade é que, _quando convergem_,\n",
    "eles vão diminuindo o erro do resultado,\n",
    "o que pode inclusive tornar métodos mais robustos quanto aos erros numéricos.\n",
    "(Claro que nada irá reduzir erros de medida...)\n",
    "\n",
    "O protótipo básico destes métodos é uma iteração de ponto fixo:\n",
    "$$ x _ {n+1} = f(x_n) = Cx_n + d,$$\n",
    "onde $C$ e $d$ dependem dos $A$ e $b$ originais.\n",
    "Para que a iteração convirja, é preciso que ela seja uma _contração_, ou seja, que\n",
    "$$\\lVert f(y) - f(z) \\rVert \\lt \\lVert y - z\\rVert$$\n",
    "para **todos** $y$ e $z$.\n",
    "No caso de sistemas lineares como acima, basta que a matriz $C$ seja uma contração;\n",
    "ou seja, que seus valores singulares sejam todos menores do que um."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-condicionadores\n",
    "\n",
    "O problema do método acima é que ele não diz como construir $C$ e $d$ em função de $A$ e $b$.\n",
    "\n",
    "Uma tática esperta para contornar este problema é o uso de pré-condicionadores.\n",
    "Um **pré-condicionador** é uma matriz $P$, da mesma ordem de $A$,\n",
    "e que usamos para \"decompor\" $A$ em duas partes: $P$ e \"o resto\", $A - P$.\n",
    "\n",
    "Se $x$ satisfaz $Ax = b$, temos também que\n",
    "$$ Px = (P - A + A)x = (P - A)x + b, $$\n",
    "e ao inverter $P$ temos um protótipo de iteração de ponto fixo:\n",
    "$$ x = P^{-1} (P - A) x + P^{-1}b .$$\n",
    "\n",
    "Todo o valor de um método de pré-condicionador está na facilidade (e estabilidade!)\n",
    "para calcular as inversas de $P$ necessárias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeiro exemplo: Jacobi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre as matrizes mais simples de se inverter estão as matrizes diagonais.\n",
    "\n",
    "O **método de Jacobi** corresponde a decompor $A$ em sua parte diagonal $D$ e \"no resto\".\n",
    "O método associado será relativamente restritivo, não podendo ser aplicado a muitos casos,\n",
    "mas tem a vantagem de ser bastante simples.\n",
    "\n",
    "### Teorema\n",
    "\n",
    "Se a matriz $A$ possuir uma diagonal dominante de linhas\n",
    "(ou seja, $a _ {ii}$ é maior do que a soma dos outros $a _ {ij}$)\n",
    "o método de Jacobi converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Implemente este algoritmo,\n",
    "e pense sobre o critério de parada para o mesmo.\n",
    "Observe a velocidade de convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo exemplo: gradientes conjugados\n",
    "\n",
    "Re-escrevendo a fórmula de ponto fixo, temos:\n",
    "\n",
    "$$ x _ {n+1} = P^{-1} (P - A) x_n + P^{-1}b = (I -  P^{-1} A) x_n + P^{-1}b = x_n + P^{-1} A x_n + P^{-1}b = x_n + v_n,$$\n",
    "onde podemos interpretar $v_n$ como sendo uma \"correção\" de $x_n$,\n",
    "na direção de ficar mais próximo da solução real $x$ do sistema\n",
    "(se o método estiver convergindo, claro!).\n",
    "\n",
    "Vamos agora focalizar em matrizes simétricas positivas definidas.\n",
    "Uma idéia interessante neste caso é buscar $v_n$ perpendiculares entre si,\n",
    "para evitar ficar \"zigue-zagueando\" demais.\n",
    "Além disso, se todos os $v_n$ forem ortogonais entre si,\n",
    "e cada um corrigir \"a quantidade certa\",\n",
    "teremos exatamente $N$ passos, onde $N$ é a dimensão de $A$.\n",
    "Garantir convergência em um número finito de etapas é melhor ainda!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seja $x_0$ dado (pode ser inclusive $x_0 = 0$...), e formemos o resto $r_0 = b - Ax_0$.\n",
    "Vamos andar com $x$ na direção $p_0 = r_0$.\n",
    "Se quisermos minimizar o erro na direção $p_0$, temos a seguinte situação:\n",
    "$$\\begin{align*}\n",
    "     x_1 & = x_0 + \\alpha p_0 \\\\\n",
    "b - Ax_1 & = b - Ax_0 - \\alpha Ap_0 \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "A nova direção do erro será dada por $p_1 = r_1 - \\beta p_0$,\n",
    "onde $\\beta$ será escolhido para que $p_1$ seja ortogonal a $p_0$.\n",
    "\n",
    "E, em seguida, repetimos o processo.\n",
    "Isto poderia ser feito com qualquer matriz A.\n",
    "Mas, se $A$ é simétrica positiva definida,\n",
    "não apenas os $p _ {i+1}$ são ortogonais a $p_i$ (por construção),\n",
    "mas também a todos os anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.3.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
